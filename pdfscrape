#! /usr/bin/env python3

import argparse
import os
import datetime
from urllib.parse import urljoin
from getpass import getpass
import requests
import bs4
import pytz
from dateutil.parser import parse

#Todo: accept file as input, containing urls, triggered by flag
#Todo: instead of url, read from stdin
#Todo: handle case when different pdfs have the same name, only differ in url
#Todo: auth via OAuth?
#Todo: instead of keyword, accept regex
HEADER = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36', 
          "Upgrade-Insecure-Requests": "1",
          "DNT": "1",
          "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
          "Accept-Language": "en-US,en;q=0.5",
          "Accept-Encoding": "gzip, deflate"}
cred = None

#=======Acquire and filter===================================

def get_pdflinks(url, filetype):
    '''Gathers all pdflinks from url'''
    response = None
    try:
        response = requests.get(url, headers=HEADER)
        response.raise_for_status()
    except requests.exceptions.HTTPError as err:
        print(err)
        exit(1)
    soup = bs4.BeautifulSoup(response.content, 'html.parser')
    
    #default case is pdf
    if filetype is None:
        filetype = ".pdf"
    else:
        filetype = "." + filetype
    links = []
    for e in soup.find_all('a'):
        try:
            if e.get('href').endswith(filetype):
                links.append(urljoin(url, e.get('href')))
        except AttributeError:
            continue
    return links

# Set of filter functions
def filter_links_for_keyword(links, keyword):
    return [x for x in links if keyword in x.split('/')[-1]]
def filter_links_for_updatable(links):
    return [x for x in links if nonexistent_or_updatable(x)]

# decides for an url, if the pdf at that url should be downloaded or not
def nonexistent_or_updatable(linktofile):
    filename = linktofile.split('/')[-1]
    if os.path.isfile(filename):

        filetime = datetime.datetime.fromtimestamp(os.path.getmtime(filename))
        zone = pytz.timezone('Europe/Zurich')
        filetime = zone.localize(filetime)
        r = None
        try:
            r = requests.head(linktofile, auth=cred)
            r.raise_for_status()
        except requests.exceptions.HTTPError as err:
            print(err)
            return False
        urltime = parse(r.headers['Last-Modified'])

        if urltime < filetime:
            print(filename + " already exists and is up-to-date")
            return False
    return True


#=======Print or download======================================

def print_links(links):
    print("Found " + str(len(links)) + " pdf files:")
    for l in links:
        print(l.split('/')[-1])

def download_links(links, cred=None):
    numSucc = 0
    for l in links:

        filename = l.split('/')[-1]
        content = None
        try:
            r = requests.get(l, auth=cred)
            r.raise_for_status()
            content = r.content
        except requests.exceptions.HTTPError as err:
            print(err)
            print(filename + " could not be downloaded")
            continue

        print("Successfully downloaded " + filename)
        numSucc = numSucc + 1
        pdf = open(filename, 'wb')
        pdf.write(content)
        pdf.close()

    print("Successful downloads: " + str(numSucc) + " out of " + str(len(links)))
    return numSucc

#=======Main====================================================
def main():
    parser = argparse.ArgumentParser("Scrape all pdfs from a given URL")
    parser.add_argument('url', help="URL of the website to scrape")
    parser.add_argument('-i', '--interactive', action='store_true', help="Use Credentials interactive")
    parser.add_argument('-k', '--keyword', help="Only fetch pdfs with this keyword in its name")
    parser.add_argument('-a', '--available', action='store_true', help="Only list all targets")
    parser.add_argument('-t', '--type', help="use different file ending (default: .pdf)")
    args = parser.parse_args()

    links = get_pdflinks(args.url, args.type)

    if args.interactive:
        user = input("Username: ")
        passwd = getpass()
        cred = (user, passwd)

    if args.keyword is not None:
        links = filter_links_for_keyword(links, args.keyword)
   
    if args.available:
        print_links(links)
    else:
        #before downloading, make sure duplicate names on the site are handled
        links = filter_links_for_updatable(links)
        download_links(links)
    
   
if __name__ == "__main__":
    main()
