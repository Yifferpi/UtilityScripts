#! /usr/bin/env python3

import argparse
import requests
import bs4
import os
import datetime
import pytz
from dateutil.parser import parse
from urllib.parse import urljoin
from getpass import getpass

#Todo: accept file as input, containing urls, triggered by flag
#Todo: instead of url, read from stdin
#Todo: handle case when different pdfs have the same name, only differ in url

#=======Arguments================================
parser = argparse.ArgumentParser("Scrape all pdfs from a given URL")
parser.add_argument('url', help="URL of the website to scrape")
parser.add_argument('-i', '--interactive', action='store_true', help="Use Credentials interactive")
parser.add_argument('-k', '--keyword', help="Only fetch pdfs with this keyword in its name")
parser.add_argument('-a', '--available', action='store_true', help="Only list all pdfs that would be downloaded")
args = parser.parse_args()

#=======Acquire and filter===================================

def get_pdflinks(url):
    response = None
    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.exceptions.HTTPError as err:
        print(err)
        exit(1)
    soup = bs4.BeautifulSoup(response.content, 'html.parser')

    links = []
    for e in soup.find_all('a'):
        if e.get('href').endswith(".pdf"):
            links.append(urljoin(url, e.get('href')))
    return links

# Set of filter functions
def filter_links_for_keyword(links, keyword):
    links = [x for x in links if keyword in x.split('/')[-1]]
def filter_links_for_updatable(links):
    links = [x for x in links if nonexistent_or_updatable(x)]

# decides for an url, if the pdf at that url should be downloaded or not
def nonexistent_or_updatable(linktofile):
    filename = linktofile.split('/')[-1]
    if os.path.isfile(filename):

        filetime = datetime.datetime.fromtimestamp(os.path.getmtime(filename))
        zone = pytz.timezone('Europe/Zurich')
        filetime = zone.localize(filetime)
        r = None
        try:
            if cred is None:
                r = requests.head(linktofile)
            else:
                r = requests.head(linktofile, auth=cred)
            r.raise_for_status()
        except requests.exceptions.HTTPError as err:
            print(err)
            return False
        urltime = parse(r.headers['Last-Modified'])

        if urltime < filetime:
            print(filename + " already exists and is up-to-date")
            return False
    return True


#=======Print or download======================================

def print_links(links):
    print("Found " + str(len(links)) + " pdf files:")
    for l in links:
        print(l.split('/')[-1])

def download_links(links, cred=None):
    numSucc = 0
    for l in links:

        filename = l.split('/')[-1]
        content = None
        try:
            content = download_file(l, cred)
        except requests.exceptions.HTTPError as err:
            print(err)
            print(filename + " could not be downloaded")
            continue

        print("Successfully downloaded " + filename)
        numSucc = numSucc + 1
        pdf = open(filename, 'wb')
        pdf.write(content)
        pdf.close()

    print("Successful downloads: " + str(numSucc) + " out of " + str(len(links)))
    return numSucc

#this should be simplified to not use an if statement.
def download_file(link, cred=None):
    r = None
    if cred is None:
        r = requests.get(link)
    else:
        r = requests.get(link, auth=cred)
    r.raise_for_status()
    return r.content

#=======Main====================================================
def main():

    cred = None
    links = get_pdflinks(args.url)

    if args.interactive:
        user = input("Username: ")
        passwd = getpass()
        cred = (user, passwd)

    if args.keyword is not None:
        filter_links_for_keyword(links, args.keyword)
   
    if args.available:
        print_links(links)
    else:
        #before downloading, make sure duplicate names on the site are handled
        filter_links_for_updatable(links)
        download_links(links)
    
   
if __name__ == "__main__":
    main()
